#!/usr/bin/env python3
"""
VALIDA√á√ÉO FINAL - FASE 2 PASSO 2
Sistema de Recomenda√ß√µes de Investimentos

Este script executa uma valida√ß√£o completa e robusta da implementa√ß√£o
da Fase 2 at√© o Passo 2, verificando se todos os componentes est√£o
funcionando conforme especificado.

VERIFICA√á√ïES:
‚úì Passo 2.1: Algoritmo de Scoring
‚úì Passo 2.2: Benchmarking Setorial  
‚úì Passo 2.3: Sistema de Crit√©rios de Qualidade
‚úì Integra√ß√£o com Arquitetura Existente
‚úì Compatibilidade com Agno Framework

Data: 13/07/2025
Autor: Claude Sonnet 4
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Setup do projeto
PROJECT_ROOT = Path.cwd()
sys.path.insert(0, str(PROJECT_ROOT))

class ValidationResult(Enum):
    PASS = "‚úÖ PASS"
    FAIL = "‚ùå FAIL"
    WARN = "‚ö†Ô∏è  WARN"
    INFO = "‚ÑπÔ∏è  INFO"

@dataclass
class ValidationTest:
    name: str
    description: str
    category: str
    required: bool = True
    result: Optional[ValidationResult] = None
    details: str = ""
    score: float = 0.0

class Fase2Passo2Validator:
    """
    Validador completo da Fase 2 Passo 2
    
    Verifica se a implementa√ß√£o do Sistema de Scoring Fundamentalista
    est√° completa e funcionando conforme especifica√ß√£o.
    """
    
    def __init__(self):
        self.tests: List[ValidationTest] = []
        self.start_time = time.time()
        
        # Contadores
        self.total_score = 0.0
        self.max_score = 0.0
        self.critical_failures = 0
        
        self._print_header()
    
    def _print_header(self):
        """Imprime cabe√ßalho do validador"""
        print("üîç VALIDADOR COMPLETO - FASE 2 PASSO 2")
        print("=" * 80)
        print("Sistema de Scoring Fundamentalista")
        print("Data:", datetime.now().strftime("%d/%m/%Y %H:%M:%S"))
        print("=" * 80)
        print()
        print("ESPECIFICA√á√ÉO FASE 2 - PASSO 2:")
        print("   2.1 ‚úì Algoritmo de Scoring (Sistema de pesos, normaliza√ß√£o, score 0-100)")
        print("   2.2 ‚úì Benchmarking Setorial (Percentis, ranking, compara√ß√µes)")
        print("   2.3 ‚úì Crit√©rios de Qualidade (Quality tiers, filtros, ROE>15%)")
        print("=" * 80)
    
    def add_test(self, test: ValidationTest):
        """Adiciona teste √† lista"""
        self.tests.append(test)
        self.max_score += 10.0 if test.required else 5.0
    
    def run_test(self, test_func, test: ValidationTest) -> ValidationResult:
        """Executa um teste individual"""
        print(f"\nüîç {test.name}")
        print(f"   üìã {test.description}")
        
        try:
            result, details, score = test_func()
            test.result = result
            test.details = details
            test.score = score
            
            if test.required and result == ValidationResult.FAIL:
                self.critical_failures += 1
            
            self.total_score += score
            
            print(f"   {result.value}: {details}")
            if score > 0:
                print(f"   üìä Score: {score:.1f}/10")
            
            return result
            
        except Exception as e:
            test.result = ValidationResult.FAIL
            test.details = f"Erro na execu√ß√£o: {str(e)}"
            test.score = 0.0
            
            if test.required:
                self.critical_failures += 1
            
            print(f"   {ValidationResult.FAIL.value}: {test.details}")
            return ValidationResult.FAIL
    
    # ================================================================
    # TESTES DO PASSO 2.1: ALGORITMO DE SCORING
    # ================================================================
    
    def test_fundamental_scoring_system_exists(self) -> Tuple[ValidationResult, str, float]:
        """Verifica se o sistema principal existe e est√° completo"""
        scoring_file = PROJECT_ROOT / "agents" / "analyzers" / "fundamental_scoring_system.py"
        
        if not scoring_file.exists():
            return ValidationResult.FAIL, "Arquivo fundamental_scoring_system.py n√£o encontrado", 0.0
        
        try:
            content = scoring_file.read_text(encoding='utf-8')
            
            # Componentes obrigat√≥rios
            required_components = {
                "FundamentalAnalyzerAgent": "class FundamentalAnalyzerAgent",
                "ScoringEngine": "class ScoringEngine", 
                "FundamentalScore": "class FundamentalScore",
                "QualityTier": "class QualityTier",
                "ScoringWeights": "class ScoringWeights",
                "composite_score": "composite_score"
            }
            
            found = 0
            missing = []
            
            for name, pattern in required_components.items():
                if pattern in content:
                    found += 1
                else:
                    missing.append(name)
            
            score = (found / len(required_components)) * 10
            
            if missing:
                return ValidationResult.FAIL, f"Componentes faltando: {', '.join(missing)}", score
            else:
                return ValidationResult.PASS, f"Todos os componentes encontrados ({len(content):,} chars)", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro ao ler arquivo: {e}", 0.0
    
    def test_scoring_engine_implementation(self) -> Tuple[ValidationResult, str, float]:
        """Testa implementa√ß√£o do ScoringEngine"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import ScoringEngine, ScoringWeights
            
            # Testar cria√ß√£o
            weights = ScoringWeights()
            engine = ScoringEngine(weights)
            
            # M√©todos obrigat√≥rios
            required_methods = [
                'calculate_valuation_score',
                'calculate_profitability_score', 
                'calculate_growth_score',
                'calculate_composite_score'
            ]
            
            found_methods = [m for m in required_methods if hasattr(engine, m)]
            score = (len(found_methods) / len(required_methods)) * 10
            
            if len(found_methods) == len(required_methods):
                return ValidationResult.PASS, f"ScoringEngine completo ({len(found_methods)} m√©todos)", score
            else:
                missing = [m for m in required_methods if m not in found_methods]
                return ValidationResult.FAIL, f"M√©todos faltando: {', '.join(missing)}", score
            
        except ImportError:
            return ValidationResult.FAIL, "Erro de import do ScoringEngine", 0.0
        except Exception as e:
            return ValidationResult.FAIL, f"Erro na implementa√ß√£o: {e}", 0.0
    
    def test_scoring_calculation(self) -> Tuple[ValidationResult, str, float]:
        """Testa se o c√°lculo de score funciona"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import FundamentalAnalyzerAgent
            
            agent = FundamentalAnalyzerAgent()
            result = agent.analyze_single_stock("PETR4")
            
            if "error" in result:
                return ValidationResult.WARN, "An√°lise retornou erro (esperado em dev)", 5.0
            
            # Verificar estrutura
            required_fields = ['fundamental_score', 'recommendation', 'stock_code']
            missing = [f for f in required_fields if f not in result]
            
            if missing:
                return ValidationResult.FAIL, f"Campos faltando: {', '.join(missing)}", 0.0
            
            # Verificar score
            fs = result['fundamental_score']
            if 'composite_score' not in fs:
                return ValidationResult.FAIL, "composite_score n√£o encontrado", 0.0
            
            score = fs['composite_score']
            if not (0 <= score <= 100):
                return ValidationResult.FAIL, f"Score fora do range 0-100: {score}", 0.0
            
            return ValidationResult.PASS, f"C√°lculo funcionando (Score: {score:.1f})", 10.0
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro no c√°lculo: {e}", 0.0
    
    # ================================================================
    # TESTES DO PASSO 2.2: BENCHMARKING SETORIAL
    # ================================================================
    
    def test_sector_comparison(self) -> Tuple[ValidationResult, str, float]:
        """Testa compara√ß√£o setorial"""
        try:
            scoring_file = PROJECT_ROOT / "agents" / "analyzers" / "fundamental_scoring_system.py"
            content = scoring_file.read_text(encoding='utf-8')
            
            # Verificar campos de percentil
            sector_components = {
                "sector_percentile": "sector_percentile" in content,
                "sector_rank": "sector_rank" in content,
                "overall_percentile": "overall_percentile" in content,
                "overall_rank": "overall_rank" in content
            }
            
            found = sum(sector_components.values())
            score = (found / len(sector_components)) * 10
            
            if found == len(sector_components):
                return ValidationResult.PASS, "Todos os campos de ranking implementados", score
            elif found >= 2:
                return ValidationResult.WARN, f"Ranking parcial ({found}/{len(sector_components)})", score
            else:
                return ValidationResult.FAIL, "Sistema de ranking n√£o implementado", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro no teste setorial: {e}", 0.0
    
    def test_percentile_calculation(self) -> Tuple[ValidationResult, str, float]:
        """Testa c√°lculo de percentis"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import FundamentalScore
            
            # Verificar se FundamentalScore tem campos necess√°rios
            score_fields = FundamentalScore.__annotations__.keys()
            
            percentile_fields = ['sector_percentile', 'overall_percentile']
            found_fields = [f for f in percentile_fields if f in score_fields]
            
            score = (len(found_fields) / len(percentile_fields)) * 10
            
            if len(found_fields) == len(percentile_fields):
                return ValidationResult.PASS, "Estrutura de percentis implementada", score
            else:
                missing = [f for f in percentile_fields if f not in found_fields]
                return ValidationResult.FAIL, f"Campos faltando: {', '.join(missing)}", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro nos percentis: {e}", 0.0
    
    # ================================================================
    # TESTES DO PASSO 2.3: CRIT√âRIOS DE QUALIDADE
    # ================================================================
    
    def test_quality_tier_system(self) -> Tuple[ValidationResult, str, float]:
        """Testa sistema de quality tier"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import QualityTier
            
            # Verificar tiers obrigat√≥rios
            expected_tiers = ['EXCELLENT', 'GOOD', 'AVERAGE', 'BELOW_AVERAGE', 'POOR']
            actual_tiers = [tier.name for tier in QualityTier]
            
            found = len([tier for tier in expected_tiers if tier in actual_tiers])
            score = (found / len(expected_tiers)) * 10
            
            if found == len(expected_tiers):
                return ValidationResult.PASS, f"Sistema de qualidade completo ({found} tiers)", score
            else:
                missing = [tier for tier in expected_tiers if tier not in actual_tiers]
                return ValidationResult.FAIL, f"Tiers faltando: {', '.join(missing)}", score
            
        except ImportError:
            return ValidationResult.FAIL, "QualityTier n√£o encontrado", 0.0
        except Exception as e:
            return ValidationResult.FAIL, f"Erro no sistema de qualidade: {e}", 0.0
    
    def test_quality_filters(self) -> Tuple[ValidationResult, str, float]:
        """Testa filtros de qualidade (ROE>15%, etc.)"""
        try:
            scoring_file = PROJECT_ROOT / "agents" / "analyzers" / "fundamental_scoring_system.py"
            content = scoring_file.read_text(encoding='utf-8')
            
            # Verificar crit√©rios de qualidade
            quality_criteria = {
                "ROE": ("roe" in content.lower() or "return on equity" in content.lower()),
                "Crescimento": ("growth" in content.lower()),
                "Endividamento": ("debt" in content.lower() or "divida" in content.lower()),
                "Margens": ("margin" in content.lower())
            }
            
            found = sum(quality_criteria.values())
            score = (found / len(quality_criteria)) * 10
            
            if found >= 3:
                return ValidationResult.PASS, f"Crit√©rios implementados ({found}/{len(quality_criteria)})", score
            elif found >= 1:
                return ValidationResult.WARN, f"Crit√©rios parciais ({found}/{len(quality_criteria)})", score
            else:
                return ValidationResult.FAIL, "Crit√©rios de qualidade n√£o encontrados", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro nos filtros: {e}", 0.0
    
    # ================================================================
    # TESTES DE INFRAESTRUTURA
    # ================================================================
    
    def test_agno_integration(self) -> Tuple[ValidationResult, str, float]:
        """Testa integra√ß√£o com Agno Framework"""
        try:
            from agno.agent import Agent
            
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import FundamentalAnalyzerAgent
            
            if issubclass(FundamentalAnalyzerAgent, Agent):
                return ValidationResult.PASS, "Heran√ßa do Agent correta", 5.0
            else:
                return ValidationResult.FAIL, "FundamentalAnalyzerAgent n√£o herda de Agent", 0.0
            
        except ImportError:
            return ValidationResult.WARN, "Agno n√£o dispon√≠vel (esperado em dev)", 3.0
        except Exception as e:
            return ValidationResult.FAIL, f"Erro na integra√ß√£o: {e}", 0.0
    
    def test_database_integration(self) -> Tuple[ValidationResult, str, float]:
        """Testa integra√ß√£o com database"""
        try:
            from database.repositories import get_stock_repository
            
            repo = get_stock_repository()
            stocks = repo.get_all_stocks()
            
            return ValidationResult.PASS, f"Database integrado ({len(stocks)} a√ß√µes)", 5.0
            
        except ImportError:
            return ValidationResult.WARN, "Database n√£o dispon√≠vel (esperado em dev)", 3.0
        except Exception as e:
            return ValidationResult.WARN, f"Database com problemas: {e}", 2.0
    
    def test_project_structure(self) -> Tuple[ValidationResult, str, float]:
        """Testa estrutura do projeto"""
        required_dirs = [
            "agents/analyzers",
            "database", 
            "utils",
            "config"
        ]
        
        existing_dirs = [d for d in required_dirs if (PROJECT_ROOT / d).exists()]
        score = (len(existing_dirs) / len(required_dirs)) * 5
        
        if len(existing_dirs) == len(required_dirs):
            return ValidationResult.PASS, f"Estrutura completa ({len(existing_dirs)} dirs)", score
        else:
            missing = [d for d in required_dirs if d not in existing_dirs]
            return ValidationResult.WARN, f"Dirs faltando: {', '.join(missing)}", score
    
    # ================================================================
    # EXECU√á√ÉO DOS TESTES
    # ================================================================
    
    def run_all_tests(self):
        """Executa todos os testes de valida√ß√£o"""
        
        # TESTES CR√çTICOS DO PASSO 2.1
        print("\nüìä VALIDANDO PASSO 2.1: ALGORITMO DE SCORING")
        print("-" * 60)
        
        tests_2_1 = [
            ValidationTest("Sistema Principal", "Verificar se fundamental_scoring_system.py existe", "2.1", True),
            ValidationTest("ScoringEngine", "Verificar implementa√ß√£o do motor de scoring", "2.1", True),
            ValidationTest("C√°lculo de Score", "Testar funcionalidade do c√°lculo", "2.1", True)
        ]
        
        for test in tests_2_1:
            self.add_test(test)
        
        self.run_test(self.test_fundamental_scoring_system_exists, tests_2_1[0])
        self.run_test(self.test_scoring_engine_implementation, tests_2_1[1])
        self.run_test(self.test_scoring_calculation, tests_2_1[2])
        
        # TESTES DO PASSO 2.2
        print("\nüìà VALIDANDO PASSO 2.2: BENCHMARKING SETORIAL")
        print("-" * 60)
        
        tests_2_2 = [
            ValidationTest("Compara√ß√£o Setorial", "Verificar campos de ranking", "2.2", True),
            ValidationTest("C√°lculo de Percentis", "Verificar estrutura de percentis", "2.2", True)
        ]
        
        for test in tests_2_2:
            self.add_test(test)
        
        self.run_test(self.test_sector_comparison, tests_2_2[0])
        self.run_test(self.test_percentile_calculation, tests_2_2[1])
        
        # TESTES DO PASSO 2.3
        print("\nüèÜ VALIDANDO PASSO 2.3: CRIT√âRIOS DE QUALIDADE")
        print("-" * 60)
        
        tests_2_3 = [
            ValidationTest("Sistema Quality Tier", "Verificar enums de qualidade", "2.3", True),
            ValidationTest("Filtros de Qualidade", "Verificar crit√©rios ROE, crescimento", "2.3", True)
        ]
        
        for test in tests_2_3:
            self.add_test(test)
        
        self.run_test(self.test_quality_tier_system, tests_2_3[0])
        self.run_test(self.test_quality_filters, tests_2_3[1])
        
        # TESTES DE INFRAESTRUTURA
        print("\nüîß VALIDANDO INFRAESTRUTURA")
        print("-" * 60)
        
        infra_tests = [
            ValidationTest("Estrutura do Projeto", "Verificar diret√≥rios", "Infra", False),
            ValidationTest("Integra√ß√£o Database", "Verificar conex√£o com banco", "Infra", False),
            ValidationTest("Integra√ß√£o Agno", "Verificar heran√ßa do Agent", "Infra", False)
        ]
        
        for test in infra_tests:
            self.add_test(test)
        
        self.run_test(self.test_project_structure, infra_tests[0])
        self.run_test(self.test_database_integration, infra_tests[1])
        self.run_test(self.test_agno_integration, infra_tests[2])
    
    def generate_final_report(self) -> Dict[str, Any]:
        """Gera relat√≥rio final completo"""
        
        execution_time = time.time() - self.start_time
        success_rate = (self.total_score / self.max_score * 100) if self.max_score > 0 else 0
        
        # Determinar status geral
        if self.critical_failures == 0 and success_rate >= 80:
            overall_status = "‚úÖ FASE 2 PASSO 2 COMPLETO"
            status_icon = "üéâ"
        elif self.critical_failures <= 1 and success_rate >= 60:
            overall_status = "‚ö†Ô∏è  FASE 2 PASSO 2 QUASE COMPLETO"
            status_icon = "üìù"
        else:
            overall_status = "‚ùå FASE 2 PASSO 2 INCOMPLETO"
            status_icon = "üîß"
        
        # Agrupar testes por categoria
        tests_by_category = {}
        for test in self.tests:
            if test.category not in tests_by_category:
                tests_by_category[test.category] = []
            tests_by_category[test.category].append(test)
        
        return {
            "overall_status": overall_status,
            "status_icon": status_icon,
            "execution_time": execution_time,
            "summary": {
                "total_tests": len(self.tests),
                "critical_failures": self.critical_failures,
                "total_score": self.total_score,
                "max_score": self.max_score,
                "success_rate": success_rate
            },
            "tests_by_category": {
                category: [
                    {
                        "name": test.name,
                        "description": test.description,
                        "required": test.required,
                        "result": test.result.value if test.result else "NOT_RUN",
                        "details": test.details,
                        "score": test.score
                    }
                    for test in tests
                ]
                for category, tests in tests_by_category.items()
            },
            "next_steps": self._generate_next_steps(overall_status),
            "timestamp": datetime.now().isoformat()
        }
    
    def _generate_next_steps(self, status: str) -> List[str]:
        """Gera pr√≥ximos passos baseado no status"""
        steps = []
        
        if "COMPLETO" in status:
            steps.extend([
                "üéØ FASE 2 PASSO 2 FINALIZADO COM SUCESSO!",
                "",
                "üìã PR√ìXIMOS PASSOS DA FASE 2:",
                "   ‚Ä¢ Implementar Passo 3: Agente Analisador Core",
                "   ‚Ä¢ Configurar Pipeline de Processamento",
                "   ‚Ä¢ Implementar Sistema de Justificativas Autom√°ticas",
                "   ‚Ä¢ Criar APIs de An√°lise Fundamentalista",
                "   ‚Ä¢ Configurar Sistema de Cache Inteligente",
                "",
                "üöÄ PREPARA√á√ÉO PARA FASE 3:",
                "   ‚Ä¢ Sistema de Recomenda√ß√µes baseado em Scoring",
                "   ‚Ä¢ Integra√ß√£o com An√°lise T√©cnica",
                "   ‚Ä¢ Dashboard de Monitoramento"
            ])
        
        elif "QUASE" in status:
            # Identificar problemas cr√≠ticos
            critical_failed = [test for test in self.tests if test.required and test.result == ValidationResult.FAIL]
            
            steps.extend([
                "üîß CORRE√á√ïES NECESS√ÅRIAS:",
                ""
            ])
            
            for test in critical_failed:
                steps.append(f"   ‚Ä¢ {test.name}: {test.details}")
            
            steps.extend([
                "",
                "üìù AP√ìS CORRE√á√ïES:",
                "   ‚Ä¢ Executar novamente este validador",
                "   ‚Ä¢ Prosseguir para Passo 3 se aprovado"
            ])
        
        else:
            steps.extend([
                "‚ùå IMPLEMENTA√á√ÉO INCOMPLETA",
                "",
                "üîß A√á√ïES OBRIGAT√ìRIAS:",
                "   ‚Ä¢ Implementar fundamental_scoring_system.py completo",
                "   ‚Ä¢ Garantir heran√ßa do Agent (Agno)",
                "   ‚Ä¢ Implementar ScoringEngine com todos os m√©todos",
                "   ‚Ä¢ Adicionar sistema de Quality Tiers",
                "   ‚Ä¢ Configurar campos de percentil setorial",
                "",
                "üìö CONSULTAR DOCUMENTA√á√ÉO:",
                "   ‚Ä¢ Fase2.md - Especifica√ß√£o completa",
                "   ‚Ä¢ Arquitetura do projeto existente",
                "   ‚Ä¢ Exemplos nos outros sistemas de scoring"
            ])
        
        return steps
    
    def print_final_report(self):
        """Imprime relat√≥rio final formatado"""
        
        report = self.generate_final_report()
        
        print("\n" + "=" * 80)
        print("üìã RELAT√ìRIO FINAL - VALIDA√á√ÉO FASE 2 PASSO 2")
        print("=" * 80)
        
        print(f"\n{report['status_icon']} STATUS GERAL: {report['overall_status']}")
        
        summary = report['summary']
        print(f"\nüìä RESUMO EXECUTIVO:")
        print(f"   ‚Ä¢ Total de Testes: {summary['total_tests']}")
        print(f"   ‚Ä¢ Falhas Cr√≠ticas: {summary['critical_failures']}")
        print(f"   ‚Ä¢ Score Total: {summary['total_score']:.1f}/{summary['max_score']:.1f}")
        print(f"   ‚Ä¢ Taxa de Sucesso: {summary['success_rate']:.1f}%")
        print(f"   ‚Ä¢ Tempo de Execu√ß√£o: {report['execution_time']:.2f}s")
        
        # Relat√≥rio por categoria
        print(f"\nüìà RESULTADOS POR CATEGORIA:")
        for category, tests in report['tests_by_category'].items():
            print(f"\n   üìÇ {category}:")
            for test in tests:
                status_emoji = "‚úÖ" if "PASS" in test['result'] else "‚ö†Ô∏è" if "WARN" in test['result'] else "‚ùå"
                required_mark = "üî¥" if test['required'] else "üîµ"
                print(f"      {status_emoji} {required_mark} {test['name']}: {test['score']:.1f}/10")
                if test['details']:
                    print(f"         ‚îî‚îÄ {test['details']}")
        
        # Pr√≥ximos passos
        print(f"\nüìã PR√ìXIMOS PASSOS:")
        for step in report['next_steps']:
            print(step)
        
        print("\n" + "=" * 80)
        
        # Salvar relat√≥rio
        report_file = PROJECT_ROOT / f"validation_report_fase2_passo2_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Relat√≥rio detalhado salvo: {report_file.name}")
        
        return report['summary']['success_rate'] >= 60 and report['summary']['critical_failures'] <= 1

def main():
    """Fun√ß√£o principal do validador"""
    
    try:
        validator = Fase2Passo2Validator()
        
        # Executar todos os testes
        validator.run_all_tests()
        
        # Gerar e imprimir relat√≥rio final
        success = validator.print_final_report()
        
        # C√≥digo de sa√≠da
        if success:
            print("\nüéâ VALIDA√á√ÉO CONCLU√çDA COM SUCESSO!")
            print("‚úÖ Fase 2 Passo 2 est√° pronta para produ√ß√£o")
            sys.exit(0)
        else:
            print("\n‚ö†Ô∏è  VALIDA√á√ÉO CONCLU√çDA COM PROBLEMAS")
            print("üîß Corre√ß√µes necess√°rias antes de prosseguir")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  Valida√ß√£o interrompida pelo usu√°rio")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nüí• ERRO CR√çTICO NO VALIDADOR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
