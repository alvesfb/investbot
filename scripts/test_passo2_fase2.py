#!/usr/bin/env python3
"""
VALIDAÃ‡ÃƒO FINAL - FASE 2 PASSO 2
Sistema de RecomendaÃ§Ãµes de Investimentos

Este script executa uma validaÃ§Ã£o completa e robusta da implementaÃ§Ã£o
da Fase 2 atÃ© o Passo 2, verificando se todos os componentes estÃ£o
funcionando conforme especificado.

VERIFICAÃ‡Ã•ES:
âœ“ Passo 2.1: Algoritmo de Scoring
âœ“ Passo 2.2: Benchmarking Setorial  
âœ“ Passo 2.3: Sistema de CritÃ©rios de Qualidade
âœ“ IntegraÃ§Ã£o com Arquitetura Existente
âœ“ Compatibilidade com Agno Framework

Data: 13/07/2025
Autor: Claude Sonnet 4
"""

import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Setup do projeto
PROJECT_ROOT = Path.cwd()
sys.path.insert(0, str(PROJECT_ROOT))

class ValidationResult(Enum):
    PASS = "âœ… PASS"
    FAIL = "âŒ FAIL"
    WARN = "âš ï¸  WARN"
    INFO = "â„¹ï¸  INFO"

@dataclass
class ValidationTest:
    name: str
    description: str
    category: str
    required: bool = True
    result: Optional[ValidationResult] = None
    details: str = ""
    score: float = 0.0

class Fase2Passo2Validator:
    """
    Validador completo da Fase 2 Passo 2
    
    Verifica se a implementaÃ§Ã£o do Sistema de Scoring Fundamentalista
    estÃ¡ completa e funcionando conforme especificaÃ§Ã£o.
    """
    
    def __init__(self):
        self.tests: List[ValidationTest] = []
        self.start_time = time.time()
        
        # Contadores
        self.total_score = 0.0
        self.max_score = 0.0
        self.critical_failures = 0
        
        self._print_header()
    
    def _print_header(self):
        """Imprime cabeÃ§alho do validador"""
        print("ðŸ” VALIDADOR COMPLETO - FASE 2 PASSO 2")
        print("=" * 80)
        print("Sistema de Scoring Fundamentalista")
        print("Data:", datetime.now().strftime("%d/%m/%Y %H:%M:%S"))
        print("=" * 80)
        print()
        print("ESPECIFICAÃ‡ÃƒO FASE 2 - PASSO 2:")
        print("   2.1 âœ“ Algoritmo de Scoring (Sistema de pesos, normalizaÃ§Ã£o, score 0-100)")
        print("   2.2 âœ“ Benchmarking Setorial (Percentis, ranking, comparaÃ§Ãµes)")
        print("   2.3 âœ“ CritÃ©rios de Qualidade (Quality tiers, filtros, ROE>15%)")
        print("=" * 80)
    
    def add_test(self, test: ValidationTest):
        """Adiciona teste Ã  lista"""
        self.tests.append(test)
        self.max_score += 10.0 if test.required else 5.0
    
    def run_test(self, test_func, test: ValidationTest) -> ValidationResult:
        """Executa um teste individual"""
        print(f"\nðŸ” {test.name}")
        print(f"   ðŸ“‹ {test.description}")
        
        try:
            result, details, score = test_func()
            test.result = result
            test.details = details
            test.score = score
            
            if test.required and result == ValidationResult.FAIL:
                self.critical_failures += 1
            
            self.total_score += score
            
            print(f"   {result.value}: {details}")
            if score > 0:
                print(f"   ðŸ“Š Score: {score:.1f}/10")
            
            return result
            
        except Exception as e:
            test.result = ValidationResult.FAIL
            test.details = f"Erro na execuÃ§Ã£o: {str(e)}"
            test.score = 0.0
            
            if test.required:
                self.critical_failures += 1
            
            print(f"   {ValidationResult.FAIL.value}: {test.details}")
            return ValidationResult.FAIL
    
    # ================================================================
    # TESTES DO PASSO 2.1: ALGORITMO DE SCORING
    # ================================================================
    
    def test_fundamental_scoring_system_exists(self) -> Tuple[ValidationResult, str, float]:
        """Verifica se o sistema principal existe e estÃ¡ completo"""
        scoring_file = PROJECT_ROOT / "agents" / "analyzers" / "fundamental_scoring_system.py"
        
        if not scoring_file.exists():
            return ValidationResult.FAIL, "Arquivo fundamental_scoring_system.py nÃ£o encontrado", 0.0
        
        try:
            content = scoring_file.read_text(encoding='utf-8')
            
            # Componentes obrigatÃ³rios
            required_components = {
                "FundamentalAnalyzerAgent": "class FundamentalAnalyzerAgent",
                "ScoringEngine": "class ScoringEngine", 
                "FundamentalScore": "class FundamentalScore",
                "QualityTier": "class QualityTier",
                "ScoringWeights": "class ScoringWeights",
                "composite_score": "composite_score"
            }
            
            found = 0
            missing = []
            
            for name, pattern in required_components.items():
                if pattern in content:
                    found += 1
                else:
                    missing.append(name)
            
            score = (found / len(required_components)) * 10
            
            if missing:
                return ValidationResult.FAIL, f"Componentes faltando: {', '.join(missing)}", score
            else:
                return ValidationResult.PASS, f"Todos os componentes encontrados ({len(content):,} chars)", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro ao ler arquivo: {e}", 0.0
    
    def test_scoring_engine_implementation(self) -> Tuple[ValidationResult, str, float]:
        """Testa implementaÃ§Ã£o do ScoringEngine"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import ScoringEngine, ScoringWeights
            
            # Testar criaÃ§Ã£o
            weights = ScoringWeights()
            engine = ScoringEngine(weights)
            
            # MÃ©todos obrigatÃ³rios
            required_methods = [
                'calculate_valuation_score',
                'calculate_profitability_score', 
                'calculate_growth_score',
                'calculate_composite_score'
            ]
            
            found_methods = [m for m in required_methods if hasattr(engine, m)]
            score = (len(found_methods) / len(required_methods)) * 10
            
            if len(found_methods) == len(required_methods):
                return ValidationResult.PASS, f"ScoringEngine completo ({len(found_methods)} mÃ©todos)", score
            else:
                missing = [m for m in required_methods if m not in found_methods]
                return ValidationResult.FAIL, f"MÃ©todos faltando: {', '.join(missing)}", score
            
        except ImportError:
            return ValidationResult.FAIL, "Erro de import do ScoringEngine", 0.0
        except Exception as e:
            return ValidationResult.FAIL, f"Erro na implementaÃ§Ã£o: {e}", 0.0
    
    def test_scoring_calculation(self) -> Tuple[ValidationResult, str, float]:
        """Testa se o cÃ¡lculo de score funciona"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import FundamentalAnalyzerAgent
            
            agent = FundamentalAnalyzerAgent()
            result = agent.analyze_single_stock("PETR4")
            
            if "error" in result:
                return ValidationResult.WARN, "AnÃ¡lise retornou erro (esperado em dev)", 5.0
            
            # Verificar estrutura
            required_fields = ['fundamental_score', 'recommendation', 'stock_code']
            missing = [f for f in required_fields if f not in result]
            
            if missing:
                return ValidationResult.FAIL, f"Campos faltando: {', '.join(missing)}", 0.0
            
            # Verificar score
            fs = result['fundamental_score']
            if 'composite_score' not in fs:
                return ValidationResult.FAIL, "composite_score nÃ£o encontrado", 0.0
            
            score = fs['composite_score']
            if not (0 <= score <= 100):
                return ValidationResult.FAIL, f"Score fora do range 0-100: {score}", 0.0
            
            return ValidationResult.PASS, f"CÃ¡lculo funcionando (Score: {score:.1f})", 10.0
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro no cÃ¡lculo: {e}", 0.0
    
    # ================================================================
    # TESTES DO PASSO 2.2: BENCHMARKING SETORIAL
    # ================================================================
    
    def test_sector_comparison(self) -> Tuple[ValidationResult, str, float]:
        """Testa comparaÃ§Ã£o setorial"""
        try:
            scoring_file = PROJECT_ROOT / "agents" / "analyzers" / "fundamental_scoring_system.py"
            content = scoring_file.read_text(encoding='utf-8')
            
            # Verificar campos de percentil
            sector_components = {
                "sector_percentile": "sector_percentile" in content,
                "sector_rank": "sector_rank" in content,
                "overall_percentile": "overall_percentile" in content,
                "overall_rank": "overall_rank" in content
            }
            
            found = sum(sector_components.values())
            score = (found / len(sector_components)) * 10
            
            if found == len(sector_components):
                return ValidationResult.PASS, "Todos os campos de ranking implementados", score
            elif found >= 2:
                return ValidationResult.WARN, f"Ranking parcial ({found}/{len(sector_components)})", score
            else:
                return ValidationResult.FAIL, "Sistema de ranking nÃ£o implementado", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro no teste setorial: {e}", 0.0
    
    def test_percentile_calculation(self) -> Tuple[ValidationResult, str, float]:
        """Testa cÃ¡lculo de percentis"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import FundamentalScore
            
            # Verificar se FundamentalScore tem campos necessÃ¡rios
            score_fields = FundamentalScore.__annotations__.keys()
            
            percentile_fields = ['sector_percentile', 'overall_percentile']
            found_fields = [f for f in percentile_fields if f in score_fields]
            
            score = (len(found_fields) / len(percentile_fields)) * 10
            
            if len(found_fields) == len(percentile_fields):
                return ValidationResult.PASS, "Estrutura de percentis implementada", score
            else:
                missing = [f for f in percentile_fields if f not in found_fields]
                return ValidationResult.FAIL, f"Campos faltando: {', '.join(missing)}", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro nos percentis: {e}", 0.0
    
    # ================================================================
    # TESTES DO PASSO 2.3: CRITÃ‰RIOS DE QUALIDADE
    # ================================================================
    
    def test_quality_tier_system(self) -> Tuple[ValidationResult, str, float]:
        """Testa sistema de quality tier"""
        try:
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import QualityTier
            
            # Verificar tiers obrigatÃ³rios
            expected_tiers = ['EXCELLENT', 'GOOD', 'AVERAGE', 'BELOW_AVERAGE', 'POOR']
            actual_tiers = [tier.name for tier in QualityTier]
            
            found = len([tier for tier in expected_tiers if tier in actual_tiers])
            score = (found / len(expected_tiers)) * 10
            
            if found == len(expected_tiers):
                return ValidationResult.PASS, f"Sistema de qualidade completo ({found} tiers)", score
            else:
                missing = [tier for tier in expected_tiers if tier not in actual_tiers]
                return ValidationResult.FAIL, f"Tiers faltando: {', '.join(missing)}", score
            
        except ImportError:
            return ValidationResult.FAIL, "QualityTier nÃ£o encontrado", 0.0
        except Exception as e:
            return ValidationResult.FAIL, f"Erro no sistema de qualidade: {e}", 0.0
    
    def test_quality_filters(self) -> Tuple[ValidationResult, str, float]:
        """Testa filtros de qualidade (ROE>15%, etc.)"""
        try:
            scoring_file = PROJECT_ROOT / "agents" / "analyzers" / "fundamental_scoring_system.py"
            content = scoring_file.read_text(encoding='utf-8')
            
            # Verificar critÃ©rios de qualidade
            quality_criteria = {
                "ROE": ("roe" in content.lower() or "return on equity" in content.lower()),
                "Crescimento": ("growth" in content.lower()),
                "Endividamento": ("debt" in content.lower() or "divida" in content.lower()),
                "Margens": ("margin" in content.lower())
            }
            
            found = sum(quality_criteria.values())
            score = (found / len(quality_criteria)) * 10
            
            if found >= 3:
                return ValidationResult.PASS, f"CritÃ©rios implementados ({found}/{len(quality_criteria)})", score
            elif found >= 1:
                return ValidationResult.WARN, f"CritÃ©rios parciais ({found}/{len(quality_criteria)})", score
            else:
                return ValidationResult.FAIL, "CritÃ©rios de qualidade nÃ£o encontrados", score
            
        except Exception as e:
            return ValidationResult.FAIL, f"Erro nos filtros: {e}", 0.0
    
    # ================================================================
    # TESTES DE INFRAESTRUTURA
    # ================================================================
    
    def test_agno_integration(self) -> Tuple[ValidationResult, str, float]:
        """Testa integraÃ§Ã£o com Agno Framework"""
        try:
            from agno.agent import Agent
            
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            from fundamental_scoring_system import FundamentalAnalyzerAgent
            
            if issubclass(FundamentalAnalyzerAgent, Agent):
                return ValidationResult.PASS, "HeranÃ§a do Agent correta", 5.0
            else:
                return ValidationResult.FAIL, "FundamentalAnalyzerAgent nÃ£o herda de Agent", 0.0
            
        except ImportError:
            return ValidationResult.WARN, "Agno nÃ£o disponÃ­vel (esperado em dev)", 3.0
        except Exception as e:
            return ValidationResult.FAIL, f"Erro na integraÃ§Ã£o: {e}", 0.0
    
    def test_database_integration(self) -> Tuple[ValidationResult, str, float]:
        """Testa integraÃ§Ã£o com database"""
        try:
            from database.repositories import get_stock_repository
            
            repo = get_stock_repository()
            stocks = repo.get_all_stocks()
            
            return ValidationResult.PASS, f"Database integrado ({len(stocks)} aÃ§Ãµes)", 5.0
            
        except ImportError:
            return ValidationResult.WARN, "Database nÃ£o disponÃ­vel (esperado em dev)", 3.0
        except Exception as e:
            return ValidationResult.WARN, f"Database com problemas: {e}", 2.0
    
    def test_project_structure(self) -> Tuple[ValidationResult, str, float]:
        """Testa estrutura do projeto"""
        required_dirs = [
            "agents/analyzers",
            "database", 
            "utils",
            "config"
        ]
        
        existing_dirs = [d for d in required_dirs if (PROJECT_ROOT / d).exists()]
        score = (len(existing_dirs) / len(required_dirs)) * 5
        
        if len(existing_dirs) == len(required_dirs):
            return ValidationResult.PASS, f"Estrutura completa ({len(existing_dirs)} dirs)", score
        else:
            missing = [d for d in required_dirs if d not in existing_dirs]
            return ValidationResult.WARN, f"Dirs faltando: {', '.join(missing)}", score
    
    # ================================================================
    # EXECUÃ‡ÃƒO DOS TESTES
    # ================================================================
    
    def run_all_tests(self):
        """Executa todos os testes de validaÃ§Ã£o"""
        
        # TESTES CRÃTICOS DO PASSO 2.1
        print("\nðŸ“Š VALIDANDO PASSO 2.1: ALGORITMO DE SCORING")
        print("-" * 60)
        
        tests_2_1 = [
            ValidationTest("Sistema Principal", "Verificar se fundamental_scoring_system.py existe", "2.1", True),
            ValidationTest("ScoringEngine", "Verificar implementaÃ§Ã£o do motor de scoring", "2.1", True),
            ValidationTest("CÃ¡lculo de Score", "Testar funcionalidade do cÃ¡lculo", "2.1", True)
        ]
        
        for test in tests_2_1:
            self.add_test(test)
        
        self.run_test(self.test_fundamental_scoring_system_exists, tests_2_1[0])
        self.run_test(self.test_scoring_engine_implementation, tests_2_1[1])
        self.run_test(self.test_scoring_calculation, tests_2_1[2])
        
        # TESTES DO PASSO 2.2
        print("\nðŸ“ˆ VALIDANDO PASSO 2.2: BENCHMARKING SETORIAL")
        print("-" * 60)
        
        tests_2_2 = [
            ValidationTest("ComparaÃ§Ã£o Setorial", "Verificar campos de ranking", "2.2", True),
            ValidationTest("CÃ¡lculo de Percentis", "Verificar estrutura de percentis", "2.2", True)
        ]
        
        for test in tests_2_2:
            self.add_test(test)
        
        self.run_test(self.test_sector_comparison, tests_2_2[0])
        self.run_test(self.test_percentile_calculation, tests_2_2[1])
        
        # TESTES DO PASSO 2.3
        print("\nðŸ† VALIDANDO PASSO 2.3: CRITÃ‰RIOS DE QUALIDADE")
        print("-" * 60)
        
        tests_2_3 = [
            ValidationTest("Sistema Quality Tier", "Verificar enums de qualidade", "2.3", True),
            ValidationTest("Filtros de Qualidade", "Verificar critÃ©rios ROE, crescimento", "2.3", True)
        ]
        
        for test in tests_2_3:
            self.add_test(test)
        
        self.run_test(self.test_quality_tier_system, tests_2_3[0])
        self.run_test(self.test_quality_filters, tests_2_3[1])
        
        # TESTES DE INFRAESTRUTURA
        print("\nðŸ”§ VALIDANDO INFRAESTRUTURA")
        print("-" * 60)
        
        infra_tests = [
            ValidationTest("Estrutura do Projeto", "Verificar diretÃ³rios", "Infra", False),
            ValidationTest("IntegraÃ§Ã£o Database", "Verificar conexÃ£o com banco", "Infra", False),
            ValidationTest("IntegraÃ§Ã£o Agno", "Verificar heranÃ§a do Agent", "Infra", False)
        ]
        
        for test in infra_tests:
            self.add_test(test)
        
        self.run_test(self.test_project_structure, infra_tests[0])
        self.run_test(self.test_database_integration, infra_tests[1])
        self.run_test(self.test_agno_integration, infra_tests[2])
    
    def generate_final_report(self) -> Dict[str, Any]:
        """Gera relatÃ³rio final completo"""
        
        execution_time = time.time() - self.start_time
        success_rate = (self.total_score / self.max_score * 100) if self.max_score > 0 else 0
        
        # Determinar status geral
        if self.critical_failures == 0 and success_rate >= 80:
            overall_status = "âœ… FASE 2 PASSO 2 COMPLETO"
            status_icon = "ðŸŽ‰"
        elif self.critical_failures <= 1 and success_rate >= 60:
            overall_status = "âš ï¸  FASE 2 PASSO 2 QUASE COMPLETO"
            status_icon = "ðŸ“"
        else:
            overall_status = "âŒ FASE 2 PASSO 2 INCOMPLETO"
            status_icon = "ðŸ”§"
        
        # Agrupar testes por categoria
        tests_by_category = {}
        for test in self.tests:
            if test.category not in tests_by_category:
                tests_by_category[test.category] = []
            tests_by_category[test.category].append(test)
        
        return {
            "overall_status": overall_status,
            "status_icon": status_icon,
            "execution_time": execution_time,
            "summary": {
                "total_tests": len(self.tests),
                "critical_failures": self.critical_failures,
                "total_score": self.total_score,
                "max_score": self.max_score,
                "success_rate": success_rate
            },
            "tests_by_category": {
                category: [
                    {
                        "name": test.name,
                        "description": test.description,
                        "required": test.required,
                        "result": test.result.value if test.result else "NOT_RUN",
                        "details": test.details,
                        "score": test.score
                    }
                    for test in tests
                ]
                for category, tests in tests_by_category.items()
            },
            "next_steps": self._generate_next_steps(overall_status),
            "timestamp": datetime.now().isoformat()
        }
    
    def _generate_next_steps(self, status: str) -> List[str]:
        """Gera prÃ³ximos passos baseado no status"""
        steps = []
        
        if "COMPLETO" in status:
            steps.extend([
                "ðŸŽ¯ FASE 2 PASSO 2 FINALIZADO COM SUCESSO!",
                "",
                "ðŸ“‹ PRÃ“XIMOS PASSOS DA FASE 2:",
                "   â€¢ Implementar Passo 3: Agente Analisador Core",
                "   â€¢ Configurar Pipeline de Processamento",
                "   â€¢ Implementar Sistema de Justificativas AutomÃ¡ticas",
                "   â€¢ Criar APIs de AnÃ¡lise Fundamentalista",
                "   â€¢ Configurar Sistema de Cache Inteligente",
                "",
                "ðŸš€ PREPARAÃ‡ÃƒO PARA FASE 3:",
                "   â€¢ Sistema de RecomendaÃ§Ãµes baseado em Scoring",
                "   â€¢ IntegraÃ§Ã£o com AnÃ¡lise TÃ©cnica",
                "   â€¢ Dashboard de Monitoramento"
            ])
        
        elif "QUASE" in status:
            # Identificar problemas crÃ­ticos
            critical_failed = [test for test in self.tests if test.required and test.result == ValidationResult.FAIL]
            
            steps.extend([
                "ðŸ”§ CORREÃ‡Ã•ES NECESSÃRIAS:",
                ""
            ])
            
            for test in critical_failed:
                steps.append(f"   â€¢ {test.name}: {test.details}")
            
            steps.extend([
                "",
                "ðŸ“ APÃ“S CORREÃ‡Ã•ES:",
                "   â€¢ Executar novamente este validador",
                "   â€¢ Prosseguir para Passo 3 se aprovado"
            ])
        
        else:
            steps.extend([
                "âŒ IMPLEMENTAÃ‡ÃƒO INCOMPLETA",
                "",
                "ðŸ”§ AÃ‡Ã•ES OBRIGATÃ“RIAS:",
                "   â€¢ Implementar fundamental_scoring_system.py completo",
                "   â€¢ Garantir heranÃ§a do Agent (Agno)",
                "   â€¢ Implementar ScoringEngine com todos os mÃ©todos",
                "   â€¢ Adicionar sistema de Quality Tiers",
                "   â€¢ Configurar campos de percentil setorial",
                "",
                "ðŸ“š CONSULTAR DOCUMENTAÃ‡ÃƒO:",
                "   â€¢ Fase2.md - EspecificaÃ§Ã£o completa",
                "   â€¢ Arquitetura do projeto existente",
                "   â€¢ Exemplos nos outros sistemas de scoring"
            ])
        
        return steps
    
    def print_final_report(self):
        """Imprime relatÃ³rio final formatado"""
        
        report = self.generate_final_report()
        
        print("\n" + "=" * 80)
        print("ðŸ“‹ RELATÃ“RIO FINAL - VALIDAÃ‡ÃƒO FASE 2 PASSO 2")
        print("=" * 80)
        
        print(f"\n{report['status_icon']} STATUS GERAL: {report['overall_status']}")
        
        summary = report['summary']
        print(f"\nðŸ“Š RESUMO EXECUTIVO:")
        print(f"   â€¢ Total de Testes: {summary['total_tests']}")
        print(f"   â€¢ Falhas CrÃ­ticas: {summary['critical_failures']}")
        print(f"   â€¢ Score Total: {summary['total_score']:.1f}/{summary['max_score']:.1f}")
        print(f"   â€¢ Taxa de Sucesso: {summary['success_rate']:.1f}%")
        print(f"   â€¢ Tempo de ExecuÃ§Ã£o: {report['execution_time']:.2f}s")
        
        # RelatÃ³rio por categoria
        print(f"\nðŸ“ˆ RESULTADOS POR CATEGORIA:")
        for category, tests in report['tests_by_category'].items():
            print(f"\n   ðŸ“‚ {category}:")
            for test in tests:
                status_emoji = "âœ…" if "PASS" in test['result'] else "âš ï¸" if "WARN" in test['result'] else "âŒ"
                required_mark = "ðŸ”´" if test['required'] else "ðŸ”µ"
                print(f"      {status_emoji} {required_mark} {test['name']}: {test['score']:.1f}/10")
                if test['details']:
                    print(f"         â””â”€ {test['details']}")
        
        # PrÃ³ximos passos
        print(f"\nðŸ“‹ PRÃ“XIMOS PASSOS:")
        for step in report['next_steps']:
            print(step)
        
        print("\n" + "=" * 80)
        
        # Salvar relatÃ³rio
        report_file = PROJECT_ROOT / f"validation_report_fase2_passo2_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ RelatÃ³rio detalhado salvo: {report_file.name}")
        
        return report['summary']['success_rate'] >= 60 and report['summary']['critical_failures'] <= 1

def main():
    """FunÃ§Ã£o principal do validador"""
    
    try:
        validator = Fase2Passo2Validator()
        
        # Executar todos os testes
        validator.run_all_tests()
        
        # Gerar e imprimir relatÃ³rio final
        success = validator.print_final_report()
        
        # CÃ³digo de saÃ­da
        if success:
            print("\nðŸŽ‰ VALIDAÃ‡ÃƒO CONCLUÃDA COM SUCESSO!")
            print("âœ… Fase 2 Passo 2 estÃ¡ pronta para produÃ§Ã£o")
            sys.exit(0)
        else:
            print("\nâš ï¸  VALIDAÃ‡ÃƒO CONCLUÃDA COM PROBLEMAS")
            print("ðŸ”§ CorreÃ§Ãµes necessÃ¡rias antes de prosseguir")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ValidaÃ§Ã£o interrompida pelo usuÃ¡rio")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nðŸ’¥ ERRO CRÃTICO NO VALIDADOR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
