#!/usr/bin/env python3
"""
TESTE COMPLETO - QUALITY FILTERS
Sistema de RecomendaÃ§Ãµes de Investimentos - Fase 2 Passo 2.3

Este script executa uma validaÃ§Ã£o completa do sistema Quality Filters,
verificando se todos os critÃ©rios de qualidade estÃ£o implementados
conforme especificado na Fase 2 Passo 2.3.

FUNCIONALIDADES TESTADAS:
âœ“ Filtros de qualidade fundamentalista (ROE>15%, crescimento sustentÃ¡vel, etc.)
âœ“ IdentificaÃ§Ã£o de red flags em empresas problemÃ¡ticas  
âœ“ Sistema de recomendaÃ§Ãµes baseado em critÃ©rios objetivos
âœ“ AnÃ¡lise de consistÃªncia e geraÃ§Ã£o de relatÃ³rios
âœ“ Sistema de alertas para degradaÃ§Ã£o de qualidade

Data: 14/07/2025
Autor: Claude Sonnet 4
Status: VALIDAÃ‡ÃƒO FINAL - Fase 2 Passo 2.3
"""

import sys
import json
import time
import traceback
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Setup do projeto
PROJECT_ROOT = Path.cwd()
sys.path.insert(0, str(PROJECT_ROOT))

class TestResult(Enum):
    PASS = "âœ… PASS"
    FAIL = "âŒ FAIL"
    WARN = "âš ï¸  WARN"
    INFO = "â„¹ï¸  INFO"

@dataclass
class TestCase:
    name: str
    description: str
    category: str
    result: Optional[TestResult] = None
    details: str = ""
    score: float = 0.0
    execution_time: float = 0.0

class QualityFiltersValidator:
    """
    Validador completo do Quality Filters
    
    Executa testes abrangentes para verificar se o sistema de
    critÃ©rios de qualidade estÃ¡ implementado e funcionando corretamente.
    """
    
    def __init__(self):
        self.test_cases: List[TestCase] = []
        self.start_time = datetime.now()
        
        print("ğŸ† VALIDAÃ‡ÃƒO DO QUALITY FILTERS")
        print("=" * 80)
        print(f"ğŸ“… Data/Hora: {self.start_time.strftime('%d/%m/%Y %H:%M:%S')}")
        print(f"ğŸ“ DiretÃ³rio: {PROJECT_ROOT}")
        print("ğŸ¯ Validando Sistema de CritÃ©rios de Qualidade - Fase 2 Passo 2.3")
        print("=" * 80)
    
    def add_test(self, test_case: TestCase):
        """Adiciona um caso de teste"""
        self.test_cases.append(test_case)
    
    def run_test(self, test_func, name: str, description: str, category: str) -> TestCase:
        """Executa um teste e registra o resultado"""
        test_case = TestCase(name=name, description=description, category=category)
        start_time = time.time()
        
        try:
            result, details, score = test_func()
            test_case.result = result
            test_case.details = details
            test_case.score = score
            
        except Exception as e:
            test_case.result = TestResult.FAIL
            test_case.details = f"EXCEÃ‡ÃƒO: {str(e)}"
            test_case.score = 0.0
            print(f"âŒ ERRO CRÃTICO em {name}: {e}")
            traceback.print_exc()
        
        test_case.execution_time = time.time() - start_time
        self.add_test(test_case)
        
        return test_case
    
    # ================================================================
    # TESTES DE ESTRUTURA E IMPORTAÃ‡Ã•ES
    # ================================================================
    
    def test_file_structure(self) -> Tuple[TestResult, str, float]:
        """Verifica se o arquivo quality_filters.py existe e tem estrutura correta"""
        quality_file = PROJECT_ROOT / "agents" / "analyzers" / "quality_filters.py"
        
        if not quality_file.exists():
            return TestResult.FAIL, "Arquivo quality_filters.py nÃ£o encontrado", 0.0
        
        try:
            content = quality_file.read_text(encoding='utf-8')
            
            # Verificar componentes essenciais
            required_components = {
                "QualityFiltersEngine": "class QualityFiltersEngine" in content,
                "QualityTier": "class QualityTier" in content,
                "RecommendationType": "class RecommendationType" in content,
                "RedFlag": "class RedFlag" in content or "@dataclass" in content and "RedFlag" in content,
                "QualityFilter": "class QualityFilter" in content or "@dataclass" in content and "QualityFilter" in content,
                "analyze_quality": "def analyze_quality" in content,
                "detect_red_flags": "def _detect_red_flags" in content or "def detect_red_flags" in content,
                "quality_filters": "quality_filters" in content,
                "ROE": "ROE" in content and "15" in content  # CritÃ©rio ROE > 15%
            }
            
            found_components = sum(required_components.values())
            total_components = len(required_components)
            
            score = (found_components / total_components) * 20
            
            if found_components == total_components:
                return TestResult.PASS, f"Estrutura completa ({found_components}/{total_components})", score
            elif found_components >= 7:
                missing = [k for k, v in required_components.items() if not v]
                return TestResult.WARN, f"Estrutura quase completa - Faltam: {missing}", score
            else:
                return TestResult.FAIL, f"Estrutura incompleta ({found_components}/{total_components})", score
                
        except Exception as e:
            return TestResult.FAIL, f"Erro ao ler arquivo: {e}", 0.0
    
    def test_imports_and_classes(self) -> Tuple[TestResult, str, float]:
        """Testa se todas as importaÃ§Ãµes e classes estÃ£o funcionando"""
        try:
            # Tentar importar o mÃ³dulo principal
            sys.path.append(str(PROJECT_ROOT / "agents" / "analyzers"))
            
            from quality_filters import (
                QualityFiltersEngine,
                QualityTier,
                RecommendationType,
                create_quality_filters_engine
            )
            
            # Verificar se pode criar instÃ¢ncia
            engine = create_quality_filters_engine()
            
            # Verificar mÃ©todos principais
            required_methods = [
                'analyze_quality',
                'batch_analyze',
                'get_quality_summary'
            ]
            
            missing_methods = []
            for method in required_methods:
                if not hasattr(engine, method):
                    missing_methods.append(method)
            
            # Verificar enums
            quality_tiers = list(QualityTier)
            recommendations = list(RecommendationType)
            
            if not missing_methods and len(quality_tiers) >= 4 and len(recommendations) >= 4:
                return TestResult.PASS, "Todas as importaÃ§Ãµes e classes funcionando", 20.0
            else:
                issues = []
                if missing_methods:
                    issues.append(f"MÃ©todos faltantes: {missing_methods}")
                if len(quality_tiers) < 4:
                    issues.append(f"QualityTier incompleto ({len(quality_tiers)} tiers)")
                if len(recommendations) < 4:
                    issues.append(f"RecommendationType incompleto ({len(recommendations)} tipos)")
                
                return TestResult.WARN, f"Problemas: {'; '.join(issues)}", 15.0
                
        except ImportError as e:
            return TestResult.FAIL, f"Erro de importaÃ§Ã£o: {e}", 0.0
        except Exception as e:
            return TestResult.FAIL, f"Erro inesperado: {e}", 0.0
    
    # ================================================================
    # TESTES DE FILTROS DE QUALIDADE
    # ================================================================
    
    def test_quality_filters_implementation(self) -> Tuple[TestResult, str, float]:
        """Testa implementaÃ§Ã£o dos filtros de qualidade obrigatÃ³rios"""
        try:
            from quality_filters import create_quality_filters_engine, get_quality_filters_list
            
            engine = create_quality_filters_engine()
            filters_list = get_quality_filters_list()
            
            # Verificar filtros obrigatÃ³rios conforme especificaÃ§Ã£o
            required_filters = {
                'ROE': False,
                'crescimento': False,
                'endividamento': False,
                'margem': False
            }
            
            for filter_info in filters_list:
                filter_name = filter_info['name'].lower()
                filter_desc = filter_info['description'].lower()
                
                # ROE consistente > 15%
                if 'roe' in filter_name or 'roe' in filter_desc:
                    if filter_info['threshold'] >= 15.0:
                        required_filters['ROE'] = True
                
                # Crescimento sustentÃ¡vel
                if 'crescimento' in filter_desc or 'growth' in filter_desc or 'revenue' in filter_info['metric']:
                    required_filters['crescimento'] = True
                
                # Endividamento controlado
                if 'debt' in filter_info['metric'] or 'endiv' in filter_desc:
                    required_filters['endividamento'] = True
                
                # Margens estÃ¡veis
                if 'margin' in filter_info['metric'] or 'margem' in filter_desc:
                    required_filters['margem'] = True
            
            implemented_filters = sum(required_filters.values())
            total_required = len(required_filters)
            
            score = (implemented_filters / total_required) * 20
            
            if implemented_filters == total_required:
                return TestResult.PASS, f"Todos os filtros obrigatÃ³rios implementados ({implemented_filters}/{total_required})", score
            elif implemented_filters >= 3:
                missing = [k for k, v in required_filters.items() if not v]
                return TestResult.WARN, f"Filtros principais implementados - Faltam: {missing}", score
            else:
                return TestResult.FAIL, f"Filtros insuficientes ({implemented_filters}/{total_required})", score
                
        except Exception as e:
            return TestResult.FAIL, f"Erro no teste de filtros: {e}", 0.0
    
    def test_quality_analysis_functionality(self) -> Tuple[TestResult, str, float]:
        """Testa funcionalidade de anÃ¡lise de qualidade"""
        try:
            from quality_filters import create_quality_filters_engine, QualityTier
            
            engine = create_quality_filters_engine()
            
            # Dados de teste - empresa de alta qualidade
            high_quality_metrics = {
                'roe': 20.0,          # > 15% âœ“
                'roa': 8.0,
                'roic': 15.0,
                'revenue_growth_3y': 10.0,  # Crescimento sustentÃ¡vel âœ“
                'debt_ebitda': 2.0,    # Endividamento controlado âœ“
                'current_ratio': 1.8,
                'net_margin': 12.0,    # Margem estÃ¡vel âœ“
                'ebitda_margin': 20.0,
                'pe_ratio': 18.0
            }
            
            # Executar anÃ¡lise
            analysis = engine.analyze_quality("TEST_GOOD", high_quality_metrics)
            
            # ValidaÃ§Ãµes
            validations = [
                analysis.stock_code == "TEST_GOOD",
                analysis.quality_score > 60.0,  # Deveria ser alta
                analysis.quality_tier in [QualityTier.GOOD, QualityTier.EXCELLENT],
                analysis.filters_passed > analysis.total_filters * 0.7,  # Pelo menos 70% dos filtros
                len(analysis.red_flags) <= 2,  # Poucos red flags
                analysis.critical_red_flags == 0  # Nenhum red flag crÃ­tico
            ]
            
            passed_validations = sum(validations)
            
            if passed_validations >= 5:
                return TestResult.PASS, f"AnÃ¡lise funcionando corretamente ({passed_validations}/6 validaÃ§Ãµes)", 20.0
            elif passed_validations >= 4:
                return TestResult.WARN, f"AnÃ¡lise funcional com limitaÃ§Ãµes ({passed_validations}/6)", 15.0
            else:
                details = f"Score: {analysis.quality_score:.1f}, Tier: {analysis.quality_tier}, Filtros: {analysis.filters_passed}/{analysis.total_filters}"
                return TestResult.FAIL, f"AnÃ¡lise com problemas ({passed_validations}/6) - {details}", 8.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro na anÃ¡lise de qualidade: {e}", 0.0
    
    # ================================================================
    # TESTES DE RED FLAGS
    # ================================================================
    
    def test_red_flags_detection(self) -> Tuple[TestResult, str, float]:
        """Testa detecÃ§Ã£o de red flags crÃ­ticos"""
        try:
            from quality_filters import create_quality_filters_engine, RedFlagSeverity
            
            engine = create_quality_filters_engine()
            
            # Dados de teste - empresa com mÃºltiplos red flags
            problematic_metrics = {
                'roe': -5.0,           # Red flag crÃ­tico
                'roa': 1.0,
                'revenue_growth_3y': -15.0,  # Red flag alto
                'debt_ebitda': 8.0,    # Red flag crÃ­tico
                'current_ratio': 0.8,  # Red flag alto
                'net_margin': -3.0,    # Red flag crÃ­tico
                'ebitda_margin': 5.0,
                'pe_ratio': 50.0       # Red flag mÃ©dio
            }
            
            # Executar anÃ¡lise
            analysis = engine.analyze_quality("TEST_BAD", problematic_metrics)
            
            # ValidaÃ§Ãµes especÃ­ficas para red flags
            critical_flags = [rf for rf in analysis.red_flags if hasattr(rf, 'severity') and rf.severity.name == 'CRITICAL']
            high_flags = [rf for rf in analysis.red_flags if hasattr(rf, 'severity') and rf.severity.name == 'HIGH']
            
            validations = [
                len(analysis.red_flags) >= 3,  # Deve detectar mÃºltiplos red flags
                len(critical_flags) >= 2,      # Pelo menos 2 crÃ­ticos (ROE negativo, dÃ­vida alta, margem negativa)
                analysis.quality_score < 40.0,  # Score deve ser baixo
                analysis.critical_red_flags > 0,  # Contador de crÃ­ticos
                len(analysis.weaknesses) > len(analysis.strengths),  # Mais fraquezas que forÃ§as
                'VENDA' in analysis.recommendation.value  # RecomendaÃ§Ã£o de venda
            ]
            
            passed_validations = sum(validations)
            
            details = f"Red flags: {len(analysis.red_flags)} (crÃ­ticos: {len(critical_flags)}), Score: {analysis.quality_score:.1f}"
            
            if passed_validations >= 5:
                return TestResult.PASS, f"DetecÃ§Ã£o de red flags funcionando - {details}", 20.0
            elif passed_validations >= 4:
                return TestResult.WARN, f"DetecÃ§Ã£o parcial - {details}", 15.0
            else:
                return TestResult.FAIL, f"DetecÃ§Ã£o insuficiente ({passed_validations}/6) - {details}", 8.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro na detecÃ§Ã£o de red flags: {e}", 0.0
    
    def test_red_flags_types(self) -> Tuple[TestResult, str, float]:
        """Testa tipos especÃ­ficos de red flags conforme especificaÃ§Ã£o"""
        try:
            from quality_filters import get_red_flag_types
            
            red_flag_types = get_red_flag_types()
            
            # Red flags obrigatÃ³rios conforme especificaÃ§Ã£o
            required_red_flags = {
                'roe_negativo': False,
                'endividamento_excessivo': False,
                'margem_negativa': False,
                'queda_receita': False
            }
            
            for rf_type in red_flag_types:
                rf_name = rf_type['name'].lower()
                rf_desc = rf_type['description'].lower()
                
                # ROE negativo
                if 'roe' in rf_name and ('negativ' in rf_name or 'negativ' in rf_desc):
                    required_red_flags['roe_negativo'] = True
                
                # Endividamento excessivo
                if ('endiv' in rf_name or 'debt' in rf_name) and ('excess' in rf_desc or 'excess' in rf_name):
                    required_red_flags['endividamento_excessivo'] = True
                
                # Margem negativa
                if 'margin' in rf_name and ('negativ' in rf_desc or 'negativ' in rf_name):
                    required_red_flags['margem_negativa'] = True
                
                # Queda de receita
                if ('receita' in rf_desc or 'revenue' in rf_name) and ('queda' in rf_desc or 'growth' in rf_name):
                    required_red_flags['queda_receita'] = True
            
            implemented_flags = sum(required_red_flags.values())
            total_required = len(required_red_flags)
            
            score = (implemented_flags / total_required) * 15
            
            if implemented_flags == total_required:
                return TestResult.PASS, f"Todos os red flags obrigatÃ³rios implementados ({implemented_flags}/{total_required})", score
            elif implemented_flags >= 3:
                missing = [k for k, v in required_red_flags.items() if not v]
                return TestResult.WARN, f"Red flags principais implementados - Faltam: {missing}", score
            else:
                return TestResult.FAIL, f"Red flags insuficientes ({implemented_flags}/{total_required})", score
                
        except Exception as e:
            return TestResult.FAIL, f"Erro no teste de tipos de red flags: {e}", 0.0
    
    # ================================================================
    # TESTES DE SISTEMA DE RECOMENDAÃ‡Ã•ES
    # ================================================================
    
    def test_recommendation_system(self) -> Tuple[TestResult, str, float]:
        """Testa sistema de recomendaÃ§Ãµes baseado em critÃ©rios"""
        try:
            from quality_filters import create_quality_filters_engine, RecommendationType
            
            engine = create_quality_filters_engine()
            
            # CenÃ¡rio 1: Empresa excelente - deve recomendar COMPRA
            excellent_metrics = {
                'roe': 25.0, 'roa': 12.0, 'roic': 18.0,
                'revenue_growth_3y': 15.0, 'debt_ebitda': 1.5,
                'current_ratio': 2.0, 'net_margin': 18.0,
                'ebitda_margin': 25.0, 'pe_ratio': 15.0
            }
            
            # CenÃ¡rio 2: Empresa mÃ©dia - deve recomendar MANTER
            average_metrics = {
                'roe': 12.0, 'roa': 6.0, 'roic': 10.0,
                'revenue_growth_3y': 3.0, 'debt_ebitda': 3.5,
                'current_ratio': 1.3, 'net_margin': 8.0,
                'ebitda_margin': 15.0, 'pe_ratio': 22.0
            }
            
            # CenÃ¡rio 3: Empresa problemÃ¡tica - deve recomendar VENDA
            poor_metrics = {
                'roe': -2.0, 'roa': 1.0, 'roic': 2.0,
                'revenue_growth_3y': -10.0, 'debt_ebitda': 7.0,
                'current_ratio': 0.9, 'net_margin': -1.0,
                'ebitda_margin': 8.0, 'pe_ratio': 40.0
            }
            
            # Executar anÃ¡lises
            excellent_analysis = engine.analyze_quality("EXCELLENT", excellent_metrics)
            average_analysis = engine.analyze_quality("AVERAGE", average_metrics)
            poor_analysis = engine.analyze_quality("POOR", poor_metrics)
            
            # Validar recomendaÃ§Ãµes
            validations = [
                excellent_analysis.recommendation in [RecommendationType.BUY, RecommendationType.STRONG_BUY],
                excellent_analysis.confidence > 70.0,
                average_analysis.recommendation in [RecommendationType.HOLD, RecommendationType.BUY],
                poor_analysis.recommendation in [RecommendationType.SELL, RecommendationType.STRONG_SELL],
                poor_analysis.confidence > 60.0,
                excellent_analysis.quality_score > average_analysis.quality_score > poor_analysis.quality_score
            ]
            
            passed_validations = sum(validations)
            
            details = f"Excellent: {excellent_analysis.recommendation.value} ({excellent_analysis.confidence:.0f}%), "
            details += f"Average: {average_analysis.recommendation.value}, "
            details += f"Poor: {poor_analysis.recommendation.value} ({poor_analysis.confidence:.0f}%)"
            
            if passed_validations >= 5:
                return TestResult.PASS, f"Sistema de recomendaÃ§Ãµes funcionando - {details}", 20.0
            elif passed_validations >= 4:
                return TestResult.WARN, f"RecomendaÃ§Ãµes parciais - {details}", 15.0
            else:
                return TestResult.FAIL, f"Problemas nas recomendaÃ§Ãµes ({passed_validations}/6) - {details}", 8.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro no sistema de recomendaÃ§Ãµes: {e}", 0.0
    
    def test_confidence_calculation(self) -> Tuple[TestResult, str, float]:
        """Testa cÃ¡lculo de confidence nas recomendaÃ§Ãµes"""
        try:
            from quality_filters import create_quality_filters_engine
            
            engine = create_quality_filters_engine()
            
            # Dados de teste com diferentes nÃ­veis de qualidade
            test_cases = [
                {'metrics': {'roe': 30.0, 'debt_ebitda': 1.0, 'net_margin': 20.0, 'revenue_growth_3y': 20.0}, 'expected_confidence': 80.0},
                {'metrics': {'roe': 15.0, 'debt_ebitda': 3.0, 'net_margin': 10.0, 'revenue_growth_3y': 5.0}, 'expected_confidence': 50.0},
                {'metrics': {'roe': -5.0, 'debt_ebitda': 8.0, 'net_margin': -2.0, 'revenue_growth_3y': -15.0}, 'expected_confidence': 70.0}
            ]
            
            confidence_tests = []
            
            for i, test_case in enumerate(test_cases):
                analysis = engine.analyze_quality(f"TEST_{i}", test_case['metrics'])
                
                # Confidence deve estar no range vÃ¡lido e fazer sentido
                valid_confidence = (
                    0.0 <= analysis.confidence <= 100.0 and
                    analysis.confidence > 30.0  # MÃ­nimo de confidence para qualquer recomendaÃ§Ã£o
                )
                
                confidence_tests.append(valid_confidence)
            
            passed_tests = sum(confidence_tests)
            
            if passed_tests == len(test_cases):
                return TestResult.PASS, f"CÃ¡lculo de confidence funcionando ({passed_tests}/{len(test_cases)})", 10.0
            elif passed_tests >= 2:
                return TestResult.WARN, f"Confidence parcialmente funcional ({passed_tests}/{len(test_cases)})", 7.0
            else:
                return TestResult.FAIL, f"Problemas no cÃ¡lculo de confidence ({passed_tests}/{len(test_cases)})", 3.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro no teste de confidence: {e}", 0.0
    
    # ================================================================
    # TESTES DE FUNCIONALIDADES AVANÃ‡ADAS
    # ================================================================
    
    def test_batch_processing(self) -> Tuple[TestResult, str, float]:
        """Testa processamento em lote"""
        try:
            from quality_filters import create_quality_filters_engine
            
            engine = create_quality_filters_engine()
            
            # Dados de teste para processamento em lote
            batch_data = [
                {
                    'stock_code': 'BATCH1',
                    'metrics': {'roe': 20.0, 'debt_ebitda': 2.0, 'net_margin': 15.0, 'revenue_growth_3y': 10.0}
                },
                {
                    'stock_code': 'BATCH2', 
                    'metrics': {'roe': 10.0, 'debt_ebitda': 4.0, 'net_margin': 5.0, 'revenue_growth_3y': 2.0}
                },
                {
                    'stock_code': 'BATCH3',
                    'metrics': {'roe': -5.0, 'debt_ebitda': 8.0, 'net_margin': -2.0, 'revenue_growth_3y': -10.0}
                }
            ]
            
            # Executar processamento em lote
            batch_results = engine.batch_analyze(batch_data)
            
            # ValidaÃ§Ãµes
            validations = [
                len(batch_results) == len(batch_data),
                all(hasattr(result, 'stock_code') for result in batch_results),
                all(hasattr(result, 'quality_score') for result in batch_results),
                all(hasattr(result, 'recommendation') for result in batch_results),
                len(set(r.stock_code for r in batch_results)) == 3  # CÃ³digos Ãºnicos
            ]
            
            passed_validations = sum(validations)
            
            if passed_validations == 5:
                return TestResult.PASS, f"Processamento em lote funcionando ({len(batch_results)} empresas)", 15.0
            elif passed_validations >= 4:
                return TestResult.WARN, f"Batch parcial ({passed_validations}/5 validaÃ§Ãµes)", 10.0
            else:
                return TestResult.FAIL, f"Problemas no batch ({passed_validations}/5)", 5.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro no processamento em lote: {e}", 0.0
    
    def test_quality_summary(self) -> Tuple[TestResult, str, float]:
        """Testa geraÃ§Ã£o de resumo de qualidade"""
        try:
            from quality_filters import create_quality_filters_engine
            
            engine = create_quality_filters_engine()
            
            # Criar anÃ¡lises de teste
            test_data = [
                {'stock_code': 'GOOD1', 'metrics': {'roe': 25.0, 'debt_ebitda': 1.5, 'net_margin': 18.0}},
                {'stock_code': 'GOOD2', 'metrics': {'roe': 22.0, 'debt_ebitda': 2.0, 'net_margin': 15.0}},
                {'stock_code': 'AVG1', 'metrics': {'roe': 12.0, 'debt_ebitda': 3.5, 'net_margin': 8.0}},
                {'stock_code': 'POOR1', 'metrics': {'roe': -2.0, 'debt_ebitda': 7.0, 'net_margin': -1.0}}
            ]
            
            analyses = engine.batch_analyze(test_data)
            summary = engine.get_quality_summary(analyses)
            
            # Verificar campos essenciais do resumo
            required_fields = [
                'total_analyzed',
                'average_quality_score',
                'tier_distribution',
                'recommendation_distribution',
                'companies_with_red_flags'
            ]
            
            fields_present = sum(1 for field in required_fields if field in summary)
            
            # ValidaÃ§Ãµes adicionais
            valid_stats = (
                summary.get('total_analyzed', 0) == 4 and
                isinstance(summary.get('average_quality_score', 0), (int, float)) and
                isinstance(summary.get('tier_distribution', {}), dict) and
                isinstance(summary.get('recommendation_distribution', {}), dict)
            )
            
            if fields_present == len(required_fields) and valid_stats:
                return TestResult.PASS, f"Resumo de qualidade completo ({fields_present}/{len(required_fields)} campos)", 15.0
            elif fields_present >= 4:
                return TestResult.WARN, f"Resumo parcial ({fields_present}/{len(required_fields)})", 10.0
            else:
                return TestResult.FAIL, f"Resumo incompleto ({fields_present}/{len(required_fields)})", 5.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro no resumo de qualidade: {e}", 0.0
    
    def test_utility_functions(self) -> Tuple[TestResult, str, float]:
        """Testa funÃ§Ãµes utilitÃ¡rias"""
        try:
            from quality_filters import (
                quick_quality_check,
                get_quality_filters_list,
                get_red_flag_types
            )
            
            # Teste quick_quality_check
            test_metrics = {'roe': 18.0, 'debt_ebitda': 2.5, 'net_margin': 12.0}
            quick_result = quick_quality_check("QUICK_TEST", test_metrics)
            
            # Teste get_quality_filters_list
            filters_list = get_quality_filters_list()
            
            # Teste get_red_flag_types
            red_flags_list = get_red_flag_types()
            
            # ValidaÃ§Ãµes
            validations = [
                isinstance(quick_result, dict) and 'quality_score' in quick_result,
                isinstance(filters_list, list) and len(filters_list) > 5,
                isinstance(red_flags_list, list) and len(red_flags_list) > 5,
                'recommendation' in quick_result and 'confidence' in quick_result,
                all('name' in f and 'threshold' in f for f in filters_list),
                all('name' in rf and 'severity' in rf for rf in red_flags_list)
            ]
            
            passed_validations = sum(validations)
            
            details = f"Quick check: OK, Filtros: {len(filters_list)}, Red flags: {len(red_flags_list)}"
            
            if passed_validations == 6:
                return TestResult.PASS, f"FunÃ§Ãµes utilitÃ¡rias funcionando - {details}", 10.0
            elif passed_validations >= 4:
                return TestResult.WARN, f"Utilidades parciais ({passed_validations}/6) - {details}", 7.0
            else:
                return TestResult.FAIL, f"Problemas nas utilidades ({passed_validations}/6)", 3.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro nas funÃ§Ãµes utilitÃ¡rias: {e}", 0.0
    
    def test_alerting_system(self) -> Tuple[TestResult, str, float]:
        """Testa sistema de alertas para degradaÃ§Ã£o"""
        try:
            from quality_filters import create_quality_filters_engine, QualityAlert, QualityTier, RecommendationType
            
            # Criar anÃ¡lises simuladas
            engine = create_quality_filters_engine()
            
            # AnÃ¡lise anterior (boa qualidade)
            previous_metrics = {'roe': 20.0, 'debt_ebitda': 2.0, 'net_margin': 15.0, 'revenue_growth_3y': 10.0}
            previous_analysis = engine.analyze_quality("TEST_DEGRADATION", previous_metrics)
            
            # AnÃ¡lise atual (qualidade degradada)
            current_metrics = {'roe': -5.0, 'debt_ebitda': 8.0, 'net_margin': -2.0, 'revenue_growth_3y': -15.0}
            current_analysis = engine.analyze_quality("TEST_DEGRADATION", current_metrics)
            
            # Testar sistema de alertas
            alert_system = QualityAlert()
            alerts = alert_system.check_quality_degradation(previous_analysis, current_analysis)
            
            # ValidaÃ§Ãµes
            validations = [
                len(alerts) > 0,  # Deve detectar alertas
                any('queda' in alert.lower() or 'drop' in alert.lower() for alert in alerts),  # Alerta de queda de score
                any('red flag' in alert.lower() for alert in alerts),  # Alerta de novos red flags
                any('downgrade' in alert.lower() for alert in alerts),  # Alerta de downgrade
                isinstance(alerts, list),
                all(isinstance(alert, str) for alert in alerts)
            ]
            
            passed_validations = sum(validations)
            
            if passed_validations >= 5:
                return TestResult.PASS, f"Sistema de alertas funcionando ({len(alerts)} alertas detectados)", 10.0
            elif passed_validations >= 3:
                return TestResult.WARN, f"Alertas parciais ({passed_validations}/6)", 7.0
            else:
                return TestResult.FAIL, f"Problemas no sistema de alertas ({passed_validations}/6)", 3.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro no teste de alertas: {e}", 0.0
    
    def test_report_generation(self) -> Tuple[TestResult, str, float]:
        """Testa geraÃ§Ã£o de relatÃ³rios detalhados"""
        try:
            from quality_filters import create_quality_filters_engine, QualityReportGenerator
            
            engine = create_quality_filters_engine()
            
            # Criar anÃ¡lise de teste
            test_metrics = {'roe': 18.0, 'debt_ebitda': 2.5, 'net_margin': 12.0, 'revenue_growth_3y': 8.0}
            analysis = engine.analyze_quality("REPORT_TEST", test_metrics)
            
            # Testar gerador de relatÃ³rios
            report_gen = QualityReportGenerator()
            
            # RelatÃ³rio de empresa individual
            company_report = report_gen.generate_company_report(analysis)
            
            # RelatÃ³rio de portfÃ³lio
            portfolio_report = report_gen.generate_portfolio_report([analysis])
            
            # ValidaÃ§Ãµes do relatÃ³rio de empresa
            company_validations = [
                'company_info' in company_report,
                'quality_assessment' in company_report,
                'red_flags' in company_report,
                'strengths' in company_report,
                'weaknesses' in company_report,
                'summary' in company_report
            ]
            
            # ValidaÃ§Ãµes do relatÃ³rio de portfÃ³lio
            portfolio_validations = [
                'portfolio_summary' in portfolio_report,
                'top_performers' in portfolio_report,
                'bottom_performers' in portfolio_report,
                'report_metadata' in portfolio_report
            ]
            
            company_score = sum(company_validations)
            portfolio_score = sum(portfolio_validations)
            total_validations = company_score + portfolio_score
            max_validations = len(company_validations) + len(portfolio_validations)
            
            if total_validations >= 9:
                return TestResult.PASS, f"RelatÃ³rios completos ({total_validations}/{max_validations})", 15.0
            elif total_validations >= 7:
                return TestResult.WARN, f"RelatÃ³rios parciais ({total_validations}/{max_validations})", 10.0
            else:
                return TestResult.FAIL, f"RelatÃ³rios incompletos ({total_validations}/{max_validations})", 5.0
                
        except Exception as e:
            return TestResult.FAIL, f"Erro na geraÃ§Ã£o de relatÃ³rios: {e}", 0.0
    
    # ================================================================
    # RELATÃ“RIO FINAL
    # ================================================================
    
    def generate_final_report(self) -> Dict[str, Any]:
        """Gera relatÃ³rio final da validaÃ§Ã£o"""
        total_tests = len(self.test_cases)
        passed_tests = len([t for t in self.test_cases if t.result == TestResult.PASS])
        failed_tests = len([t for t in self.test_cases if t.result == TestResult.FAIL])
        warned_tests = len([t for t in self.test_cases if t.result == TestResult.WARN])
        
        total_score = sum(t.score for t in self.test_cases)
        max_possible_score = 195.0  # Total mÃ¡ximo baseado nos testes
        
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        score_percentage = (total_score / max_possible_score * 100) if max_possible_score > 0 else 0
        
        # Determinar status final
        if success_rate >= 85 and score_percentage >= 80:
            final_status = "âœ… COMPLETO"
        elif success_rate >= 70 and score_percentage >= 70:
            final_status = "âš ï¸  FUNCIONAL"
        elif success_rate >= 50 and score_percentage >= 50:
            final_status = "ğŸ”§ PARCIAL"
        else:
            final_status = "âŒ INCOMPLETO"
        
        execution_time = datetime.now() - self.start_time
        
        return {
            'validation_summary': {
                'status': final_status,
                'success_rate': success_rate,
                'score_percentage': score_percentage,
                'execution_time': str(execution_time).split('.')[0]
            },
            'test_results': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'warnings': warned_tests
            },
            'score_breakdown': {
                'total_score': total_score,
                'max_possible': max_possible_score,
                'percentage': score_percentage
            }
        }


def main():
    """FunÃ§Ã£o principal - executa validaÃ§Ã£o completa"""
    print("ğŸ† VALIDAÃ‡ÃƒO COMPLETA DO QUALITY FILTERS")
    print("=" * 80)
    print("Testando Fase 2 Passo 2.3 - Sistema de CritÃ©rios de Qualidade")
    print("=" * 80)
    
    validator = QualityFiltersValidator()
    
    # Lista de testes a executar
    test_suite = [
        (validator.test_file_structure, "Estrutura do Arquivo", "Verifica arquivo e componentes essenciais", "Estrutura"),
        (validator.test_imports_and_classes, "ImportaÃ§Ãµes e Classes", "Testa importaÃ§Ãµes e criaÃ§Ã£o de objetos", "Estrutura"),
        (validator.test_quality_filters_implementation, "Filtros de Qualidade", "Valida implementaÃ§Ã£o dos filtros obrigatÃ³rios", "Core"),
        (validator.test_quality_analysis_functionality, "AnÃ¡lise de Qualidade", "Testa funcionalidade principal de anÃ¡lise", "Core"),
        (validator.test_red_flags_detection, "DetecÃ§Ã£o de Red Flags", "Valida detecÃ§Ã£o de problemas crÃ­ticos", "Core"),
        (validator.test_red_flags_types, "Tipos de Red Flags", "Verifica red flags obrigatÃ³rios", "Core"),
        (validator.test_recommendation_system, "Sistema de RecomendaÃ§Ãµes", "Testa geraÃ§Ã£o de recomendaÃ§Ãµes", "RecomendaÃ§Ãµes"),
        (validator.test_confidence_calculation, "CÃ¡lculo de Confidence", "Valida cÃ¡lculo de confianÃ§a", "RecomendaÃ§Ãµes"),
        (validator.test_batch_processing, "Processamento em Lote", "Testa anÃ¡lise de mÃºltiplas empresas", "Performance"),
        (validator.test_quality_summary, "Resumo de Qualidade", "Valida geraÃ§Ã£o de estatÃ­sticas", "RelatÃ³rios"),
        (validator.test_utility_functions, "FunÃ§Ãµes UtilitÃ¡rias", "Testa funÃ§Ãµes de conveniÃªncia", "Utilidades"),
        (validator.test_alerting_system, "Sistema de Alertas", "Valida alertas de degradaÃ§Ã£o", "Monitoramento"),
        (validator.test_report_generation, "GeraÃ§Ã£o de RelatÃ³rios", "Testa criaÃ§Ã£o de relatÃ³rios detalhados", "RelatÃ³rios")
    ]
    
    print(f"\nğŸ§ª Executando {len(test_suite)} testes...\n")
    
    # Executar todos os testes
    for i, (test_func, name, description, category) in enumerate(test_suite, 1):
        print(f"[{i:2d}/{len(test_suite)}] ğŸ§ª {name}")
        print(f"        ğŸ“ {description}")
        
        test_case = validator.run_test(test_func, name, description, category)
        
        # Exibir resultado imediato
        result_icon = {
            TestResult.PASS: "âœ…",
            TestResult.WARN: "âš ï¸",
            TestResult.FAIL: "âŒ",
            TestResult.INFO: "â„¹ï¸"
        }.get(test_case.result, "â“")
        
        print(f"        {result_icon} {test_case.result.value} - {test_case.details}")
        print(f"        ğŸ¯ Score: {test_case.score:.1f} | â±ï¸  Tempo: {test_case.execution_time:.3f}s")
        print()
    
    # Gerar relatÃ³rio final
    print("=" * 80)
    print("ğŸ“Š RELATÃ“RIO FINAL - QUALITY FILTERS VALIDATION")
    print("=" * 80)
    
    report = validator.generate_final_report()
    
    # Resumo executivo
    summary = report['validation_summary']
    print(f"\nğŸ¯ STATUS FINAL: {summary['status']}")
    print(f"ğŸ“ˆ Taxa de Sucesso: {summary['success_rate']:.1f}%")
    print(f"ğŸ¯ Score Total: {summary['score_percentage']:.1f}%")
    print(f"â±ï¸  Tempo Total: {summary['execution_time']}")
    
    # Breakdown de resultados
    results = report['test_results']
    print(f"\nğŸ“Š RESULTADOS DETALHADOS:")
    print(f"   âœ… Testes Passaram: {results['passed']}")
    print(f"   âš ï¸  Testes com Aviso: {results['warnings']}")
    print(f"   âŒ Testes Falharam: {results['failed']}")
    print(f"   ğŸ“ Total de Testes: {results['total_tests']}")
    
    # Score breakdown
    scores = report['score_breakdown']
    print(f"\nğŸ¯ PONTUAÃ‡ÃƒO:")
    print(f"   Score Obtido: {scores['total_score']:.1f}")
    print(f"   Score MÃ¡ximo: {scores['max_possible']:.1f}")
    print(f"   Percentual: {scores['percentage']:.1f}%")
    
    # Testes por categoria
    print(f"\nğŸ“‚ RESULTADOS POR CATEGORIA:")
    categories = {}
    for test in validator.test_cases:
        if test.category not in categories:
            categories[test.category] = {'passed': 0, 'total': 0, 'score': 0}
        categories[test.category]['total'] += 1
        categories[test.category]['score'] += test.score
        if test.result == TestResult.PASS:
            categories[test.category]['passed'] += 1
    
    for category, stats in categories.items():
        rate = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0
        print(f"   {category}: {stats['passed']}/{stats['total']} ({rate:.0f}%) - Score: {stats['score']:.1f}")
    
    # Funcionalidades validadas
    print(f"\nâœ… FUNCIONALIDADES VALIDADAS:")
    
    functionality_status = {
        "Filtros de qualidade fundamentalista": any(t.name == "Filtros de Qualidade" and t.result == TestResult.PASS for t in validator.test_cases),
        "ROE consistente > 15%": any("ROE" in t.details and t.result == TestResult.PASS for t in validator.test_cases),
        "Crescimento sustentÃ¡vel": any("crescimento" in t.details.lower() and t.result == TestResult.PASS for t in validator.test_cases),
        "Endividamento controlado": any("endividamento" in t.details.lower() and t.result == TestResult.PASS for t in validator.test_cases),
        "Margens estÃ¡veis": any("margem" in t.details.lower() and t.result == TestResult.PASS for t in validator.test_cases),
        "Red flags crÃ­ticos": any(t.name == "DetecÃ§Ã£o de Red Flags" and t.result == TestResult.PASS for t in validator.test_cases),
        "Sistema de recomendaÃ§Ãµes": any(t.name == "Sistema de RecomendaÃ§Ãµes" and t.result == TestResult.PASS for t in validator.test_cases),
        "Processamento em lote": any(t.name == "Processamento em Lote" and t.result == TestResult.PASS for t in validator.test_cases),
        "AnÃ¡lise de confidence": any(t.name == "CÃ¡lculo de Confidence" and t.result == TestResult.PASS for t in validator.test_cases),
        "Resumos estatÃ­sticos": any(t.name == "Resumo de Qualidade" and t.result == TestResult.PASS for t in validator.test_cases),
        "Sistema de alertas": any(t.name == "Sistema de Alertas" and t.result == TestResult.PASS for t in validator.test_cases),
        "GeraÃ§Ã£o de relatÃ³rios": any(t.name == "GeraÃ§Ã£o de RelatÃ³rios" and t.result == TestResult.PASS for t in validator.test_cases)
    }
    
    for functionality, status in functionality_status.items():
        icon = "âœ…" if status else "âŒ"
        print(f"   {icon} {functionality}")
    
    # Status da Fase 2 Passo 2.3
    overall_success = summary['score_percentage'] >= 70
    
    print(f"\nğŸ¯ FASE 2 PASSO 2.3 - SISTEMA DE CRITÃ‰RIOS DE QUALIDADE:")
    if overall_success:
        print("   âœ… IMPLEMENTAÃ‡ÃƒO VALIDADA")
        print("   âœ… Filtros de qualidade fundamentalista: FUNCIONANDO")
        print("   âœ… ROE consistente > 15%: IMPLEMENTADO")
        print("   âœ… Crescimento sustentÃ¡vel: IMPLEMENTADO")
        print("   âœ… Endividamento controlado: IMPLEMENTADO") 
        print("   âœ… Margens estÃ¡veis: IMPLEMENTADO")
        print("   âœ… Red flags para empresas problemÃ¡ticas: FUNCIONANDO")
        print("   âœ… Sistema de recomendaÃ§Ãµes: OPERACIONAL")
        print("   âœ… Todos os critÃ©rios de aceitaÃ§Ã£o atendidos")
        print(f"\nğŸŠ FASE 2 PASSO 2 COMPLETAMENTE FINALIZADA!")
        print(f"ğŸš€ PRÃ“XIMO PASSO: Implementar FundamentalAnalyzerAgent (Passo 3)")
        print(f"ğŸ“ Componentes validados:")
        print(f"   âœ… agents/analyzers/scoring_engine.py")
        print(f"   âœ… agents/analyzers/sector_comparator.py") 
        print(f"   âœ… agents/analyzers/quality_filters.py")
    else:
        print("   âŒ IMPLEMENTAÃ‡ÃƒO INCOMPLETA")
        print("   ğŸ”§ Revisar e corrigir problemas identificados")
        print("   âš ï¸  NÃ£o prosseguir atÃ© resolver os issues")
    
    print("\n" + "=" * 80)
    
    return overall_success


if __name__ == "__main__":
    try:
        success = main()
        if success:
            print("ğŸ‰ VALIDAÃ‡ÃƒO CONCLUÃDA COM SUCESSO!")
            print("âœ… Quality Filters estÃ¡ implementado e funcionando corretamente.")
            print("ğŸ† Fase 2 Passo 2.3 VALIDADA - Sistema de CritÃ©rios de Qualidade COMPLETO!")
            print("ğŸš€ Pronto para avanÃ§ar para o Passo 3 da Fase 2.")
            print("\nğŸ¯ PRÃ“XIMOS PASSOS RECOMENDADOS:")
            print("   1. Implementar FundamentalAnalyzerAgent (Passo 3.1)")
            print("   2. Pipeline de processamento integrado (Passo 3.2)")
            print("   3. Sistema de justificativas automÃ¡ticas (Passo 3.3)")
            print("\nğŸ“Š RESUMO DA FASE 2 PASSO 2:")
            print("   âœ… Scoring Engine (Passo 2.1): COMPLETO")
            print("   âœ… Sector Comparator (Passo 2.2): COMPLETO")
            print("   âœ… Quality Filters (Passo 2.3): COMPLETO")
            print("\nğŸŠ FASE 2 PASSO 2 - 100% IMPLEMENTADA!")
            sys.exit(0)
        else:
            print("âš ï¸  VALIDAÃ‡ÃƒO CONCLUÃDA COM PROBLEMAS!")
            print("ğŸ”§ Revisar e corrigir os issues identificados.")
            print("âŒ NÃ£o prosseguir atÃ© resolver os problemas.")
            print("\nğŸ” AÃ‡Ã•ES RECOMENDADAS:")
            print("   1. Verificar implementaÃ§Ã£o dos filtros obrigatÃ³rios")
            print("   2. Validar sistema de red flags")
            print("   3. Testar geraÃ§Ã£o de recomendaÃ§Ãµes")
            print("   4. Executar testes individuais para debugging")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ValidaÃ§Ã£o interrompida pelo usuÃ¡rio.")
        sys.exit(2)
        
    except Exception as e:
        print(f"\nğŸ’¥ ERRO CRÃTICO na validaÃ§Ã£o: {e}")
        traceback.print_exc()
        print("\nğŸ”§ SUGESTÃ•ES PARA RESOLVER:")
        print("   1. Verificar se quality_filters.py existe no diretÃ³rio correto")
        print("   2. Verificar se todas as dependÃªncias estÃ£o instaladas")
        print("   3. Verificar se o PYTHONPATH estÃ¡ configurado corretamente")
        print("   4. Executar o teste do quality_filters.py diretamente primeiro")
        sys.exit(3)